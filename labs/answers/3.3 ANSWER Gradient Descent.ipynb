{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Instructions:\n",
    "* Go through the notebook and complete the tasks. \n",
    "* Make sure you understand the examples given. If you need help, refer to the Essential readings or the documentation link provided, or go to the discussion forum. \n",
    "* Save your notebooks when you are done.\n",
    "\n",
    "This notebook will give you the chance to implement the gradient descent algorithm to solve a linear regression problem. \n",
    "\n",
    "The main steps involved will be:\n",
    "\n",
    "    Within a loop -\n",
    "    1. Calculate the hypothesis h \n",
    "    2. Calculate the loss \n",
    "    3. Calculate the gradient descent update \n",
    "    4. stop loop when loss falls stops changing\n",
    "\n",
    "\n",
    "**Task 1:**\n",
    "\n",
    "The first task is to load the diabetes dataset (from the earlier notebook), select one feature (bmi, the 2nd indexed feature), and split it into a training set (90%) and a test set (10%). Plot the data as a scatter plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmi\n",
      "Intercept (theta 0): \n",
      " 153.73201108210046\n",
      "Coefficients (theta 1): \n",
      " [958.76368869]\n",
      "Mean squared error: 3956.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Select 10% for testing, 90% for training\n",
    "data_len = len(diabetes.target)\n",
    "nTestSamples = np.int(0.1*data_len)\n",
    "idx_test = np.arange(1, nTestSamples)\n",
    "idx_train = np.arange(idx_test[-1]+1, data_len - idx_test[-1])\n",
    "\n",
    "# extract the bmi feature\n",
    "print(diabetes.feature_names[2])\n",
    "X_diabetes = diabetes.data[:,np.newaxis,2]\n",
    "X_test = X_diabetes[idx_test]\n",
    "X_train = X_diabetes[idx_train]\n",
    "y_test = diabetes.target[idx_test]\n",
    "y_train = diabetes.target[idx_train]\n",
    "\n",
    "# (For comparison purposes, create linear regression object)\n",
    "regr = linear_model.LinearRegression()\n",
    "# Train the model using the training sets\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Intercept (theta 0): \\n', regr.intercept_)\n",
    "print('Coefficients (theta 1): \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "\n",
    "Write a suitable hypothesis function, ```h_lin```, that can handle an single feature vector and a corresponding 2D parameter (theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_lin( X, theta):\n",
    "    'X should be a data vector, theta the parameters'\n",
    "    return theta[0] + X*theta[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:**\n",
    "\n",
    "Write a suitable loss function, ```loss_L2```, that calculates the least-squares loss between a given hypothesis and the target value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3920.389441551087"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_L2( h, y ):\n",
    "    'Calculate the least-squares loss.' \n",
    "    'h is a vector of predictions by the hypothesis function, y is the target'\n",
    "    return np.mean([ (a-b)**2 for a,b in zip(h.flatten(), y)])\n",
    "\n",
    "loss_L2(h_lin(X_test,[150, 958]), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:**\n",
    "\n",
    "Write a suitable function to calculate the gradients, ```gradient_batch```, that operates on both the parameters, ```theta[0]``` (the intercept) and ```theta[1]``` (the gradient coefficient). The function is given the data X, and the target, y. \n",
    "(Remember the difference in gradient calculationn for the intercept term.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_batch(theta, X, y):\n",
    "    'Calculate the gradients of the loss function for the parameters in theta'\n",
    "    'Given data X and target y'\n",
    "    \n",
    "    grad = [0,0]\n",
    "    \n",
    "    h = h_lin(X,theta)\n",
    "    residuals = [ (a-b) for a,b in zip(h.flatten(), y)]\n",
    "    \n",
    "    grad[0] = np.mean(residuals)\n",
    "    grad[1] = X[:,0].dot(residuals)/len(y) # calculate the mean\n",
    "    \n",
    "    return np.array(grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:**\n",
    "\n",
    "Write the batch gradient descent algorithm that iteratively updates the coefficients (an initially selected set of theta values), and stops once the loss function stops changing much (i.e. by about 0.1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD stopped at loss 4197.1493924527995, with coefficients: [153.70863843 639.52517709]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set initial theta values\n",
    "theta = [100, 100]\n",
    "\n",
    "# set stopping criterion\n",
    "loss_stop_threshold = 0.1\n",
    "\n",
    "# learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "def batch_gd( X, y, theta, alpha, loss_stop_threshold):\n",
    "    # initial loss value\n",
    "    loss = loss_L2( h_lin(X,theta), y)\n",
    "    old_loss = loss+loss_stop_threshold\n",
    "\n",
    "    # loop through \n",
    "    while( abs(old_loss - loss) > loss_stop_threshold ):\n",
    "        #print(loss)\n",
    "\n",
    "        # gradient descent update rules\n",
    "        gradients = gradient_batch(theta, X, y)\n",
    "        \n",
    "        theta = theta - alpha * gradients            \n",
    "        \n",
    "        # update loss values\n",
    "        old_loss = loss\n",
    "        loss = loss_L2( h_lin(X,theta), y)\n",
    "        \n",
    "    print('GD stopped at loss %s, with coefficients: %s' % (loss,theta))\n",
    "    return theta\n",
    "        \n",
    "theta = batch_gd(X_train, y_train, theta, alpha, loss_stop_threshold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:**\n",
    "\n",
    "Train your algorithm using the training data, then test it using the test set. Compare the performance of your algorithm to the sklearn solution. Have a play with some of the parameters you can tweak, particularly learning rate and stopping criterion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (theta 0): \n",
      " 153.70863842882753\n",
      "Coefficients (theta 1): \n",
      " 639.5251770853935\n",
      "Mean squared error: 4160.31\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_pred =  h_lin( X_test, theta)\n",
    "\n",
    "# The coefficients\n",
    "print('Intercept (theta 0): \\n', theta[0])\n",
    "print('Coefficients (theta 1): \\n', theta[1])\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Plot outputs using test data\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
