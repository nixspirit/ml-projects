{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "## Answers\n",
    "\n",
    "In this lab, we will work through using cross-validation.\n",
    "\n",
    "**Task 1:**\n",
    "In the cell below, we load the iris dataset and show the indices recovered by using a built-in cross-validation function from scikit-learn. These indices can be then used for splitting our data into a training set and testing set. Note that while in this example we use StratifiedShuffleSplit, there are many other cross-validation generators in scikit-learn. You can read more about this <a href=\"https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\">here</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train indices: [9 8 4 3 6 2 7 1] test indices: [5 0]\n",
      "train indices: [1 6 8 3 0 2 4 5] test indices: [9 7]\n",
      "train indices: [8 7 3 5 1 9 4 6] test indices: [0 2]\n",
      "train indices: [5 0 9 8 6 2 3 1] test indices: [4 7]\n",
      "train indices: [5 4 6 0 1 2 7 3] test indices: [8 9]\n",
      "train indices: [8 6 2 0 7 3 1 5] test indices: [4 9]\n",
      "train indices: [7 8 5 3 9 6 4 2] test indices: [1 0]\n",
      "train indices: [4 8 5 1 6 7 9 3] test indices: [2 0]\n",
      "train indices: [8 9 0 4 6 1 5 7] test indices: [3 2]\n",
      "train indices: [6 2 4 1 7 8 9 5] test indices: [3 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#Set X a samples times features matrix, Y equal to the targets\n",
    "#use only the first 10 datapoints for this example\n",
    "X=iris.data[0:10] \n",
    "y=iris.target[0:10] \n",
    "\n",
    "#we use 10 splits, with the test size being 0.2\n",
    "cvsplt = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in cvsplt.split(X, y):\n",
    "    print(\"train indices:\", train_index, \"test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "In the cell below, we have the sample code that was used to train a k-NN classifier on the iris dataset. Your task is to use the code above to change the classification process we show below, in order to perform cross-validation on the data and obtain an estimate of the performance of the classifier with 10 neighbours on the unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.9666666666666667\n",
      "0.9666666666666667\n",
      "0.9\n",
      "1.0\n",
      "0.9333333333333333\n",
      "0.9666666666666667\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# model answer\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "iris = datasets.load_iris()\n",
    "#Set X a samples times features matrix, Y equal to the targets\n",
    "X=iris.data \n",
    "y=iris.target \n",
    "\n",
    "\n",
    "#we use 10 splits, with the test size being 0.2\n",
    "cvsplt = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in cvsplt.split(X, y):\n",
    "    #print(\"train indices:\", train_index, \"test indices:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    knn=KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred=knn.predict(X_test)\n",
    "    print(metrics.accuracy_score(y_test,y_pred))\n",
    "\n",
    "# #split to train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# #define knn classifier, with 5 neighbors and use the euclidian distance\n",
    "# knn=KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "# #define training and testing data, fit the classifier\n",
    "# knn.fit(X_train,y_train)\n",
    "# #predict values for test data based on training data\n",
    "# y_pred=knn.predict(X_test)\n",
    "# #print values\n",
    "# print(y_test) # true values\n",
    "# print(y_pred) # predicted values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 3:**  Now you are going to attempt to write your own cross-validation function to evaluate the k-NN classifier above (i.e. not using StratifiedShuffleSplit). \n",
    "\n",
    "Again, we are using a fixed distance (euclidean) and a fixed number of neighbours (10) so we do **not** need to create a validation set.\n",
    "\n",
    "Your function (see cell below) firstly splits the indices of each of our data into bins according to the number of folds (here: 5-fold).\n",
    "\n",
    "Then, you should loop through all folds, split the data into training and testing by selecting the appropriate bins (see slides on cross-validation), train on training data and save the test result as the accuracy for each fold (see list accuracy_fold). This is the list that your function should return in the end. Remember that the extend function extends a list with more values.\n",
    "\n",
    "**Note**: you might want to refer to the additional helper notebook on indexing that is provided with this topic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8666666666666667, 1.0, 0.9666666666666667, 0.9666666666666667]\n"
     ]
    }
   ],
   "source": [
    "def myCrossVal(X,y,foldK):\n",
    "    '''\n",
    "    This function performs cross validation on the sklearn \n",
    "    KNeighborsClassifier algorithm.\n",
    "    \n",
    "    inputs:\n",
    "        X - a data matrix of size (samples, features)\n",
    "        y - a label array of size (samples,)\n",
    "        foldK - number of folds\n",
    "        \n",
    "    outpus:\n",
    "        accuracy_fold - a list of foldK accuracy values\n",
    "    \n",
    "    '''\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    accuracy_fold=[] #list to store accuracies folds\n",
    "    \n",
    "    \n",
    "    #TASK: use the function np.random.permutation to generate a list of shuffled indices from in the range (0,number of data)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Creates an array of random permutation of indices between 0 and the length of the X data.\n",
    "    indices = np.random.permutation(np.arange(0,len(X),1))\n",
    "    \n",
    "    #TASK: use the function array_split to split the indices to k different bins:\n",
    "    bins = np.array_split(indices, foldK)\n",
    "    \n",
    "    #loop through folds\n",
    "    for i in range(0,foldK):\n",
    "        foldTrain=[] # list to save current indices for training\n",
    "        foldTest=[]  # list to save current indices for testing\n",
    "        #TASK: take bin i for testing, rest for training. \n",
    "        # Can use the function extend to add indices to foldTrain and foldTest\n",
    "        foldTest = bins[i]\n",
    "        \n",
    "        for j in range(0,foldK):\n",
    "            if j!=i:\n",
    "                foldTrain.extend( bins[j] )\n",
    "        \n",
    "        #train kNN classifier\n",
    "        knn=KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "        knn.fit(X[foldTrain,:],y[foldTrain])\n",
    "        #test on test data\n",
    "        y_pred=knn.predict(X[foldTest,:])\n",
    "        #append the new accuracy to your accuracy_fold list.  \n",
    "        \n",
    "        accuracy_fold.append( metrics.accuracy_score(y[foldTest],y_pred) )\n",
    "        \n",
    "    return accuracy_fold;\n",
    "    \n",
    "accuracy_fold=myCrossVal(X,y,5)\n",
    "\n",
    "print(accuracy_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Print the average accuracy and standard deviation of your results over the 5 folds. (functions ``mean`` and ``std``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy: 0.960 (std. 0.049)\n"
     ]
    }
   ],
   "source": [
    "print('average accuracy: %.3f (std. %.3f)' % (np.mean(accuracy_fold),np.std(accuracy_fold)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross validation\n",
    "Performing **nested** cross validation is slightly more elaborate, but necessary when we want to learn the best values for our parameters (e.g. here, finding the best values for the number of neighbours that k-NN should have). You can follow the examples on scikit-learn <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\">here</a> and <a href=\"https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\">here</a> to learn how to do this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
